{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tips for editing files\n",
    "you can edit files directly on terminal using `vi`/`emacs` <br>\n",
    "Or you can open a nersc jupyter section, go to the file location on terminal, then do: <br>\n",
    "`ls $PWD/{your_filename}` <br>\n",
    "next you copy the output line, on jupyerhub, click on `File` --> `Open from path`, paste your file location <br>\n",
    "In this way you can edit file as a normal text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Obiwan in seconds\n",
    "`cd $CSCRATCH`  <br>\n",
    "`mkdir Obiwan`  <br>\n",
    "`cd Obiwan`  <br>\n",
    "`git clone https://github.com/DriftingPig/obiwan_code.git`  <br>\n",
    "`git clone https://github.com/DriftingPig/obiwan_testrun.git`  <br>\n",
    "`cd obiwan_code/production_run` <br>\n",
    "`./slurm_debug.sh` <br>\n",
    "The code should start running with these commands, products can be seen in <br>\n",
    "`$CSCRATCH/Obiwan/obiwan_testrun/decals_ngc/production_run/more_rs0`<br>\n",
    "you can see outputs directly from terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTW, better to store codes in $HOME, to prevent losing them, this is just an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preperation before a production run\n",
    "first, choose a **run name** for each production run. Here I choose the name as **decals_ngc** <br>\n",
    "go to `$CSCRATCH/Obiwan/obiwan_testrun/` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### brickstat\n",
    "`brickstat` -- for keeping track of finished and unfinished bricks <br>\n",
    "The codes here works well but you need to **change the path to your own path** in the code. They should be easy to understand\n",
    ">`real_brick_lists` -- list of bricks that aims to be processed\n",
    ">>`bricks_decals_ngc.txt` aka **bricks_{run_name}.txt** is prepared at the very first stage, it is a list of bricks you want to process  <br>\n",
    ">`bricks_decals_ngc_DR9.6.4.txt` <br>\n",
    ">`bricks_decals_ngc_DR9.6.5.txt` <br>\n",
    ">`bricks_decals_ngc_DR9.6.7.txt` <br>\n",
    ">`bricks_decals_ngc_DR9.6.8.txt` these 4 with DR9.6.X are classified bricks from `bricks_decals_ngc.txt`<br>\n",
    ">The classification is made with code `brickstat/brickclassify.py`\n",
    "\n",
    ">`decals_ngc`\n",
    ">same as the **run name** you choose <br>\n",
    ">>`FinishedBricks-DR9.6.X.txt` bricks that finishs processing in this docker version <br>\n",
    ">>`UnfinishedBricks-DR9.6.X.txt` bricks that has not finished processing in this docker version **needed for production run**<br>\n",
    ">>`FinishedBricks.txt` When all the bricks are done, stack all `FinishedBricks-DR9.6.X.txt` to make this file, it will be used for future processing (just use np.vstack to do it) **needed for post processing**\n",
    "\n",
    ">`stats` a record of bricks that encountered MemoryError (Can be soved by running less brick per node; might take a long time; very few; I ignored them), or they started but haven't finished processing (not quite useful)  <br>\n",
    ">processed with `brickcheck.py` <br>\n",
    "\n",
    "> `brickclassify.py` classify the **bricks_{run_name}.txt** file to different _[legacypipe docker versions](https://hub.docker.com/r/legacysurvey/legacypipe/tags)_ **bricks_{run_name}_DR9.6.X.txt**\n",
    "\n",
    "> `brickstat.py` checking finished bricks in production run, <br>\n",
    "reading in `bricks_{run_name}_DR9.6.X.txt`,  <br>\n",
    "and output `{run_name}/FinishedBricks-DR9.6.X.txt` `{run_name}/UnfinishedBricks-DR9.6.X.txt` \n",
    "\n",
    ">`brickcheck_finished.py` computing wall time used for each brick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decals_ngc\n",
    "As mentioned before, `decals_ngc` is a **run name**, and can be called as anything <br>\n",
    "`divided_randoms` input randoms on a per brick basis,\n",
    ">`brick_{brickname}.fits` you need each of this for each birck in `brickstat/real_brick_lists/bricks_{run_name}.txt`, code will be mentioned later\n",
    "\n",
    "`meta` I store bricklist (not required) for this run & seed (used for generating divided randoms) in this folder\n",
    ">`seed.fits`  used for generating `divided_randoms/brick_{brickname}.fits`<br>\n",
    ">`bricklist.txt` bricklist for this run \n",
    "\n",
    "`production_run` running products will be stored here\n",
    ">`more_rs0` you can make multiple iterations for the same run, each iteration will also have a different name it can be called as anything\n",
    ">> structure here are the same as dr9 tractor outputs <br>\n",
    ">>`/global/cfs/cdirs/cosmo/work/legacysurvey/dr9/south` <br>\n",
    ">>check _[Legacy Survey Files](https://www.legacysurvey.org/dr9/files/)_ for more details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code structure for running Obiwan\n",
    "go to `$CSCRATCH/Obiwan/obiwan_code/production_run/` <br>\n",
    "`slurm_debug.sh` is an example file. The real file the starts a whole production run is similar to it: `slurm964_slice0.sh`<br>\n",
    "In this file, you will need to change parameters to adapt to your own production run:<br>\n",
    "<br>\n",
    "`export LEGACYVER=DR9.6.4`  \n",
    ">__[legacypipe docker version](https://hub.docker.com/r/legacysurvey/legacypipe/tags)__ used in this production run. <br>\n",
    ">for a given brickname, you can get its version by: <br>\n",
    ">`fn = \"/global/cfs/cdirs/cosmo/work/legacysurvey/dr9/south{north}/tractor/%s/tractor-%s.fits\"%(brickname[:3],brickname)` <br>\n",
    ">`ver = fits.open(fn)[0].header['LEGPIPEV']` <br>\n",
    ">versions for south includes `DR9.6.4` `DR9.6.5` `DR9.6.7` `DR9.6.8` `DR9.6.5-4-gbb698724.txt` (only 4 bricks, I count them as 9.6.5)\n",
    "\n",
    "`export rowstart=0`\n",
    ">first injected source in `brick_{brickname}.fits`\n",
    "`export name_for_run=decals_ngc`\n",
    ">name for this production run <br>\n",
    "\n",
    "`export dataset=normal`\n",
    ">running mode, `normal` is for a DR9 run, `cosmos` is a version compatible with cosmos repeats. Other modes are currently unsupported<br>\n",
    "\n",
    "`export nobj=200`\n",
    ">maximum sources injected per brick, this number can be larger than total length of `brick_{brickname}.fits` <br>\n",
    "\n",
    "`export threads=16`\n",
    ">number of threads used. number of bricks that can be processed on parallel is 64/{threads}. Don't make this number smaller than 16, otherwise it will cause memory failure: _[cori haswell](https://docs.nersc.gov/systems/cori/)_ has 128GB memory, and each bricks takes >16GB memory to process (It will be nice to try to limit the memory for each brick to <16GB, then we can make 8 bricks parallel per node!)\n",
    "\n",
    "`#only need to be set while running cosmos`<br>\n",
    "`#export cosmos_section=$1`  <br>\n",
    "`#export rsdir=rs${rowstart}_cosmos${cosmos_section}`<br>\n",
    ">These won't be used currently. cosmos repeats ranges from set 80 to set 89, hosted on `/global/cscratch1/sd/dstn/dr9-cosmos-subs/`\n",
    "\n",
    "`export rsdir=more_rs${rowstart}`<br>\n",
    ">name for this perticular iteration, call it anything you want\n",
    "\n",
    "`export TOT_SLICE=5` <br>\n",
    "`export SLICEI=0` <br>\n",
    ">I tried to submit a 95 node jobs once, and the jobs hang there for a **very long** time, wasting a lot of time. During the same time, NERSC had an email saying that they are aware of a hanging problem and is trying to solve it. In case this happens again, I decide to limit a job to a maximum of 20 nodes. For a long bricklist (like 60k bricks), this is not enough. The feature here separtes a bricklist into 5 parts, and pick one of the part to process. If doing this, you can make 5 slurm files, each with SLICEI go from 0 to 4, to fully monitor the process \n",
    "\n",
    "`export PYTHONPATH=./mpi:$PYTHONPATH` no need to change <br>\n",
    "`export topdir=$CSCRATCH/Obiwan/obiwan_testrun/` topdir for all obiwan outputs. Make a directory and tell the code where to begin  <br>\n",
    "`#export topdir=/global/cfs/cdirs/desi/users/huikong/` my direcoty for all obiwan outputs\n",
    "`export outdir=$topdir/$name_for_run/production_run/${rsdir}` production run outputs, no need to change <br>\n",
    "`mkdir -p $outdir` no need to change <br>\n",
    "\n",
    "`srun -N 15 -n 60 -c 16 shifter --module=mpich-cle6 --image=legacysurvey/legacypipe:DR9.6.4 python mpi_dr_slice.py` \n",
    ">here you submit a slurm job: `-N 15` using 15 [cori haswell nodes](https://docs.nersc.gov/systems/cori/); `-n 60` number of parallel jobs, we have 4 paralle per node (64/16), so n = 15*4 = 60; `-c 16` number of threads used for each brick; [shifter](https://docs.nersc.gov/development/shifter/how-to-use/) is a nersc version of [docker container](https://www.docker.com/), you can not install docker on nersc, you can only make a docker contain on your laptop, upload to [dockerhub](https://hub.docker.com/), and download to nersc using `shifterimg -v pull docker:image_name:latest`; `--module=mpich-cle6` a magical module that makes MPI/mpi4py works on this docker container; `--image=legacysurvey/legacypipe:DR9.6.4` check __[legacypipe docker version](https://hub.docker.com/r/legacysurvey/legacypipe/tags)__ for all available containers. `python mpi_dr_slice.py` calls a python script in this directory. This script reads in list of bricks to be processed, assign them to available workers/check which worker is available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### etc\n",
    "Once the above are done, the rest should be automatic. In case it's useful, I explain the remaining files here <br>\n",
    "`slurm964_slice0.sh` first calls `mpi_dr_slice.py`, which reads in `UnfinishedBricks-DR9.6.X.txt`, and all the nodes will start processing these bricks. For each brick, `mpi_dr_slice.py` calls `run.sh {brickname}`, and this script calls `runbrick_sim.py` and starts the whole process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Processing\n",
    "After you get all the obiwan outputs, here is the code to collect these outputs, do target selection, assign photo z, make plots. <br>\n",
    "go to `$CSCRATCH/Obiwan/obiwan_code/analysis/` <br>\n",
    "open `postrunbrick.sh` <br>\n",
    "\n",
    "You see various options here: <br>\n",
    "\n",
    "`survey-dir /global/project/projectdirs/cosmo/data/legacysurvey/dr9/` don't change this as long as you are using dr9<br>\n",
    "`outdir /global/cfs/cdirs/desi/users/huikong/decals_ngc/production_run/` This is where you put your obiwan results, in previous example this should be `$CSCRATCH/Obiwan/obiwan_testrun/$name_for_run/production_run/` <br>\n",
    "`threads 32` number of threads used for this code, 32 is reasonable<br>\n",
    "`dataset normal` same as `dataset` option in slurm script<br>\n",
    "`subsection more_rs0` same as `rsdir` in slurm script<br>\n",
    "`nobj 200` same as `nobj` in slurm script<br>\n",
    "`startid 0` same as `startid` in slurm script<br>\n",
    "`tot-slice 30` in my example of processing 90k bricks, I separate them into 30 sesctions at the beginning<br>\n",
    "`slice-idx $VARIABLE` idex of any of the 30 (tot-slice) for this run<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge subsets\n",
    "Final step, change `merge_subsets.py` accordingly, (`tot_sets`; `subsection`; `name_for_run`), run it, you will get merged results like <br>\n",
    "`/global/cfs/cdirs/desi/users/huikong/decals_ngc/production_run/subset/subset_more_rs0.fits`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs Description\n",
    "**lrg_sv3**: sources passing lrg sv3 color cut  <br>\n",
    "**sim_ra**: input source ra <br>\n",
    "**sim_dec**: input source dec <br>\n",
    "**sim_gflux**: input source g flux <br>\n",
    "**sim_rflux**: input source r flux <br>\n",
    "**sim_zflux**: input source z flux <br>\n",
    "**sim_w1**: input source w1 AB magnitude  <br>\n",
    "**id_sample**:  <br>\n",
    "**sim_redshift**: ignore this <br>\n",
    "**sim_rhalf**: input shape_r <br>\n",
    "**sim_e1**: input e1 <br>\n",
    "**sim_e2**: input e2 <br>\n",
    "**sim_bx**: input bx <br>\n",
    "**sim_by**: input by  <br>\n",
    "**angle**: angular distance to closest input source <br>\n",
    "**matched**: within 1.5” to input source <br>\n",
    "**sim_sersic_n**: input sersic number <br>\n",
    "**pz_in_mean**: photo z for input src <br>\n",
    "**pz_in_std**: photo z std for input src <br>\n",
    "**pz_in_L68**: photo z lower 68 confidence for input src <br>\n",
    "**pz_in_U68**:photo z upper 68 confidence for input src <br>\n",
    "**pz_out_mean**: photo z for output src <br>\n",
    "**pz_out_std**: photo z std for output src <br>\n",
    "**pz_out_L68**: photo z lower 68 confidence for output src <br>\n",
    "**pz_out_U68**: photo z upper 68 confidence for output src <br>\n",
    "\n",
    "Below are match to reference catalogs like <br>\n",
    "`/global/cfs/cdirs/cosmo/data/legacysurvey/dr9/south/metrics/000/reference-0009p290.fits` <br>\n",
    "That contain information of reference stars  <br>\n",
    "**star_distance**: distance to closest star  <br>\n",
    "**star_radius**: radius of closest star  <br>\n",
    "**MS_delta_ra**: delta ra to closest star <br>\n",
    "**MS_delta_dec**: delta dec to closest star <br>\n",
    "**startid**: input randoms are drawn from input catalogs, this denotes the start id of that catalog, and it should be with in startid -- startid+50 within that catalog. The corresponding randoms are here: /global/cscratch1/sd/huikong/Obiwan/dr9_LRG/obiwan_out/deep1/divided_randoms (but all the information should already in the above cards) \n",
    "\n",
    "The rest columns are the same as [tractor catalogs](https://www.legacysurvey.org/dr9/catalogs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample seed.fits to divided_randoms/brick_{brickname}.fits\n",
    "check out '$CSCRATCH/Obiwan/obiwan_code/analysis/bin/seed_maker_mpi.py' <br>\n",
    "change relavent path in this file <br>\n",
    "then, denpending on the size of your bricklist <br>\n",
    "`salloc --nodes 20 --qos interactive --time 01:00:00 --constraint haswell` <br>\n",
    "`srun -N 20 -n 320 -c 2 shifter --module=mpich-cle6 --image=legacysurvey/legacypipe:DR9.7.1 python seed_maker_mpi.py`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the making of seed.fits\n",
    "This is complicated, I haven't check my code for a long time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosmos repeats\n",
    "First, I derived extended color cut from cosmos repeats in `$CSCRATCH/Obiwan/obiwan_code/analysis/cosmos_preprocess`<br>\n",
    "`cosmos_collect.py` collect cosmos repeats, matching each two of them, record lrg_sv3 card for 10 sets in each of the set\n",
    "`cosmos_colorcut.py` derive the extended color cut<br>\n",
    "After deriving the color cut, I cut cosmos deep using this color cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape debiasing \n",
    "I have not cleaned up codes here yet, coming soon! <br>\n",
    "check `$CSCRACH/Obiwan/obiwan_code/production_run/py-fit-coadd`\n",
    "`$CSCRACH/Obiwan/obiwan_code/production_run/slurm_simdeep_elg.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More comments\n",
    "1. Obiwan utilize the dr9.6.6 version of legacypipe. These versions are very similar. To see what gets changed here, go to this directoy, and do <br>\n",
    "`grep #obiwan *.py`this prints all the added part by obiwan <br>\n",
    "Also, there's an extra file added called `unwise_sim.py` (for simulating WISE image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
